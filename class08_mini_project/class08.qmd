---
title: "Class 08 Mini Project"
author: "Olivia Chu"
format: gfm
---

In today's mini-project, we will explore a complete analysis using the unsupervised learning techniques covered in class (clustering and PCA for now).

The data itself comes from the Wisconsin Breast Cancer Diagnostic Data Set FNA breast biopsy data.

## Data Import

```{r}
# Save your input data file into your Project directory
fna.data <- "WisconsinCancer.csv"

# Complete the following code to input the data and store as wisc.df
wisc.df <- read.csv(fna.data, row.names=1)
head(wisc.df)
```

We will not be using the pathologist provided expert diagnosis, so we will be omitting this first column when creating a new data.frame.

```{r}
# We can use -1 here to remove the first column
wisc.data <- wisc.df[,-1]
head(wisc.data)
```

We will save this diagnosis data to use for later in a new vector as a factor.

```{r}
# Create diagnosis vector for later 
diagnosis <- as.factor( wisc.df[,1] )
head(diagnosis)
```

> Q1. How many observations are in this dataset?

```{r}
nrow(wisc.data)
```

There are 569 observations in this dataset.

> Q2. How many of the observations have a malignant diagnosis?

```{r}
table(wisc.df$diagnosis)
```

212 observations have a malignant (M) diagnosis.

> Q3. How many variables/features in the data are suffixed with _mean?

First, find column names.
```{r}
colnames(wisc.data)
```

Next, I need to search within the column names for the "_mean" pattern. The `grep()` function might help.

```{r}
length( grep("_mean", colnames(wisc.data)) )
```

10 variables/features in the data are suffixed with "_mean".

> Q. How many dimensions are in this dataset?

```{r}
dim(wisc.data)
```

There are 569 rows and 30 columns in this dataset.

# Principal Component Analysis

First, do we need to scale the data before PCA or not?

```{r}
# Check column means and standard deviations to determine if we need to scale
colMeans(wisc.data)

apply(wisc.data,2,sd)
```

Execute PCA with the prcomp() function on the wisc.data, scaling if appropriate, and assign the output model to wisc.pr.

```{r}
# Perform PCA on wisc.data by completing the following code
wisc.pr <- prcomp(wisc.data)
```

```{r}
# Look at summary of results
summary(wisc.pr)
```

```{r}
round( apply(wisc.data, 2, sd), 3)
```

It looks like we do need to scale. 

```{r}
# Perform PCA on wisc.data by completing the following code
wisc.pr <- prcomp( wisc.data, scale=TRUE )

# Look at summary of results
summary(wisc.pr)
```

> Q4. From your results, what proportion of the original variance is captured by the first principal components (PC1)?

The first principal component (PC1) captures 
44.27% of the original variance.

> Q5. How many principal components (PCs) are required to describe at least 70% of the original variance in the data?

3 PCs are required to describe at least 70% of the original variance in the data (captures 72%).

> Q6. How many principal components (PCs) are required to describe at least 90% of the original variance in the data?

7 PCs are required to describe at least 90% of the original variance in the data (captures 91%).

# Interpreting PCA Results

Create a biplot of the wisc.pr using the `biplot()` function.

```{r}
biplot(wisc.pr)
```

> Q7. What stands out to you about this plot? Is it easy or difficult to understand? Why?

This plot is extremely messy and difficult to analyze and understand. This is because there are too many points concentrated in one area of the plot, making it hard to differentiate which point is which.

# PC Plot

We need to make our plot of PC1 vs PC2 (a.k.a. score plot, PC-plot, etc.). The main result of PCA...

```{r}
# Scatter plot observations by components 1 and 2
plot( wisc.pr$x[, 1], wisc.pr$x[, 2], col = diagnosis,
     xlab = "PC1", ylab = "PC2" )
```

> Q8. Generate a similar plot for principal components 1 and 3. What do you notice about these plots?

```{r}
# Repeat for components 1 and 3
plot(wisc.pr$x[, 1], wisc.pr$x[, 3], col = diagnosis, 
     xlab = "PC1", ylab = "PC3")
```

In the plot with components 1 and 2, it is easier to distinguish the red from the black points since there is somewhat of a clear-cut line to show us the two clusters. In the plot with components 1 and 3, it is harder to distinguish because there are more overlapping points due to a lower variance in component 3 (as opposed to component 2).

Let's make a fancier figure using ggplot2.

```{r}
# Create a data.frame for ggplot
df <- as.data.frame(wisc.pr$x)
df$diagnosis <- diagnosis

# Load the ggplot2 package
library(ggplot2)

# Make a scatter plot colored by diagnosis
ggplot(df) + 
  aes(PC1, PC2, col=diagnosis) + 
  geom_point()
```

## Variance Explained

We can get this from the output of the `summary()` function.

```{r}
summary(wisc.pr)
```

Calculate the variance of each principal component by squaring the sdev component of wisc.pr (i.e. wisc.pr$sdev^2). Save the result as an object called pr.var.

```{r}
# Calculate variance of each component
pr.var <- wisc.pr$sdev^2
head(pr.var)
```

Calculate the variance explained by each principal component by dividing by the total variance explained of all principal components. Assign this to a variable called pve and create a plot of variance explained for each principal component.

```{r}
# Variance explained by each principal component: pve
pve <- pr.var / sum(pr.var)

# Plot variance explained for each principal component
plot(pve, xlab = "Principal Component", 
     ylab = "Proportion of Variance Explained", 
     type = "o")
```

```{r}
# Alternative scree plot of the same data, note data driven y-axis
barplot(pve, ylab = "Precent of Variance Explained",
     names.arg=paste0("PC",1:length(pve)), las=2, axes = FALSE)
axis(2, at=pve, labels=round(pve,2)*100 )
```

# Examine the PC Loadings

How much do the original variables contribute to the new PCs that we have calculated? To get this data, we can look at the `$rotation` component of the returned PCA product.

```{r}
head( wisc.pr$rotation[, 1:3] )
```

Focus on PC1.

> Q9. For the first principal component, what is the component of the loading vector (i.e. wisc.pr$rotation[,1]) for the feature concave.points_mean?

```{r}
wisc.pr$rotation["concave.points_mean",1]
```

The component of the loading vector for the feature concave.points_mean is -0.2608538.

There is a more complicated mix of variables that go together to make up PC1 - i.e. there are many of the original variables that together contribute highly to PC1.

```{r}
loadings <- as.data.frame(wisc.pr$rotation)

ggplot(loadings) + 
  aes(PC1, rownames(loadings)) + 
  geom_col()
```

> Q10. What is the minimum number of principal components required to explain 80% of the variance of the data?

```{r}
summary(wisc.pr)
```

The minimum number of principal components required to explain 80% of the variance of the data is 5 PCs (84%).

## Hierarchial Clustering

First, scale the wisc.data data and assign the result to data.scaled.

```{r}
# Scale the wisc.data data using the "scale()" function
data.scaled <- scale(wisc.data)
```

Calculate the (Euclidean) distances between all pairs of observations in the new scaled dataset and assign the result to data.dist.

```{r}
data.dist <- dist(data.scaled)
```

Create a hierarchical clustering model using complete linkage. Manually specify the method argument to hclust() and assign the results to wisc.hclust.

```{r}
wisc.hclust <- hclust(data.dist)
```

> Q11. Using the plot() and abline() functions, what is the height at which the clustering model has 4 clusters?

```{r}
plot(wisc.hclust)
abline(h=19, col="red", lty=2)
```

The height at which the clustering model has 4 clusters is 19.

Cut this tree to yield cluster membership vector with the `cutree()` function. 

```{r}
wisc.hclust.clusters <- cutree(wisc.hclust, h=19)
table(wisc.hclust.clusters)
```
We can use the `table()` function to compare the cluster membership to the actual diagnoses.

```{r}
table(wisc.hclust.clusters, diagnosis)
```

> Q12. Can you find a better cluster vs diagnoses match by cutting into a different number of clusters between 2 and 10?

```{r}
table( cutree(wisc.hclust, h=15) )
```

Using a height of 15, we can get 7 clusters.

# Using different methods

 > Q13. Which method gives your favorite results for the same data.dist dataset? Explain your reasoning.
 
The method that gives my my favorite results is the "average" method. This is because average is a good happy-medium similarity when determining cluster linkage. "Complete", which uses the largest similarity, and "single", which uses the smallest similarity, are on the opposite ends of a spectrum, which I don't really like/prefer. 

# Combine methods: PCA and Hierarchial clustering

My PCA results were interesting as they showed a separation of M and B samples along PC1.

```{r}
plot(wisc.pr$x[, 1], wisc.pr$x[, 2], col=diagnosis)
```

I want to cluster my PCA results - that is use `wisc.pr$x` as input to `hclust()`.

Try clustering in 3 PCs, that is PC1, PC2, and PC3.

```{r}
d <- dist(wisc.pr$x[, 1:3])

wisc.pr.hclust <- hclust(d, method="ward.D2")
```

And my tree result figure.

```{r}
plot(wisc.pr.hclust)
```

Let's cut this tree into two groups/clusters.

```{r}
grps <- cutree(wisc.pr.hclust, k=2)
table(grps)
```
```{r}
table(grps, diagnosis)
```

```{r}
plot(wisc.pr$x[, 1], wisc.pr$x[, 2], col=grps)
```

```{r}
plot(wisc.pr$x[,1:2], col=diagnosis)
```

> Q15. How well does the newly created model with four clusters separate out the two diagnoses?

How well do the two clusters separate the M and B diagnoses?

```{r}
table(grps, diagnosis)
```

```{r}
(179+333)/nrow(wisc.data)
```

The newly created model with 4 clusters separate out the two diagnoses about 90% of the time.